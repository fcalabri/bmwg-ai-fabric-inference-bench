<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt" ?>
<!DOCTYPE rfc [
  <!ENTITY nbsp    "&#160;">
  <!ENTITY zwsp   "&#8203;">
  <!ENTITY nbhy   "&#8209;">
  <!ENTITY wj     "&#8288;">
]>
<rfc xmlns:xi="http://www.w3.org/2001/XInclude"
     category="info"
     docName="draft-calabria-bmwg-ai-fabric-inference-bench-00"
     ipr="trust200902"
     obsoletes=""
     updates=""
     submissionType="IETF"
     xml:lang="en"
     tocInclude="true"
     tocDepth="4"
     symRefs="true"
     sortRefs="true"
     version="3">

  <front>
    <title abbrev="AI Inference Fabric Benchmarking">
      Benchmarking Methodology for AI Inference Serving Network Fabrics
    </title>
    <seriesInfo name="Internet-Draft" value="draft-calabria-bmwg-ai-fabric-inference-bench-00"/>

    <author fullname="Fernando Calabria" initials="F." surname="Calabria">
      <organization>Cisco</organization>
      <address>
        <email>fcalabri@cisco.com</email>
      </address>
    </author>

    <author fullname="Carlos Pignataro" initials="C." surname="Pignataro">
      <organization>Blue Fern Consulting</organization>
      <address>
        <email>carlos@bluefern.consulting</email>
      </address>
    </author>

    <date day="24" month="February" year="2026"/>

    <area>Operations and Management</area>
    <workgroup>Benchmarking Methodology Working Group</workgroup>

    <keyword>benchmarking</keyword>
    <keyword>AI</keyword>
    <keyword>inference</keyword>
    <keyword>LLM</keyword>
    <keyword>network fabric</keyword>
    <keyword>RDMA</keyword>
    <keyword>KV cache</keyword>
    <keyword>MoE</keyword>
    <keyword>disaggregated serving</keyword>

    <abstract>
      <t>
        This document defines benchmarking terminology, methodologies, and Key
        Performance Indicators (KPIs) for evaluating Ethernet-based AI
        inference serving network fabrics. As Large Language Model (LLM)
        inference deployments scale to disaggregated prefill/decode
        architectures spanning hundreds or thousands of accelerators
        (GPUs/XPUs), the interconnect fabric becomes the critical bottleneck
        determining Time to First Token (TTFT), Inter-Token Latency (ITL), and
        aggregate throughput in tokens per second (TPS). This document
        establishes vendor-independent, reproducible test procedures for
        benchmarking fabric-level performance under realistic AI inference
        workloads.
      </t>
      <t>
        Coverage includes RDMA-based KV cache transfer between disaggregated
        prefill and decode workers, Mixture-of-Experts (MoE) expert
        parallelism AllToAll communication, request routing and load balancing
        for inference serving, congestion management under bursty inference
        traffic patterns, and scale/soak testing. The methodology enables
        apples-to-apples comparison across implementations, NIC transport
        stacks (RoCEv2, UET), and fabric architectures.
      </t>
      <t>
        This document is a companion to
        draft-calabria-bmwg-ai-fabric-training-bench-00, which addresses
        training workloads, and the associated terminology document that SHOULD
        be consulted before applying these methodologies.
      </t>
    </abstract>
  </front>

  <middle>

    <section anchor="introduction" numbered="true" toc="default">
      <name>Introduction</name>
      <t>
        Large Language Model (LLM) inference serving has emerged as a dominant
        consumer of datacenter network capacity, with fundamentally different
        fabric requirements compared to training workloads. While training
        workloads are characterized by bulk synchronous collective operations
        (AllReduce, AllGather) with predictable periodicity, inference
        workloads exhibit bursty, latency-sensitive request/response patterns
        with strict Service Level Objectives (SLOs) on per-token latency and
        time-to-first-token.
      </t>
      <t>
        The advent of disaggregated serving architectures, where the
        computationally intensive prefill phase (prompt processing) is
        physically separated from the memory-bound decode phase (token
        generation), introduces a new class of fabric-critical data movement:
        KV cache transfer. A single large prompt processed by a typical
        large-scale model generates multiple gigabytes of KV cache state that
        must be transferred from prefill workers to decode workers within a
        fraction of the target TTFT SLO.
      </t>
      <t>
        At cluster scale with thousands of concurrent requests, this creates
        sustained multi-terabyte-per-second aggregate transfer demands on the
        fabric. Simultaneously, Mixture-of-Experts (MoE) architectures
        introduce expert parallelism (EP), which distributes expert
        sub-networks across GPUs and requires AllToAll communication for
        token-to-expert routing. Wide EP configurations (e.g., 96-way EP
        across 12 nodes of 8 GPUs each) generate fine-grained,
        latency-sensitive inter-node traffic that contends with KV cache
        transfers on shared fabric links.
      </t>
      <t>
        This document defines vendor-independent benchmarking methodologies
        for evaluating how well a network fabric supports these
        inference-specific traffic patterns. All tests are designed for
        controlled laboratory environments using either hardware traffic
        generators or software workload emulators capable of reproducing
        inference serving traffic profiles.
      </t>

      <section anchor="requirements-language" numbered="true" toc="default">
        <name>Requirements Language</name>
        <t>
          The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
          "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
          document are to be interpreted as described in
          <xref target="RFC2119"/> and <xref target="RFC8174"/>.
        </t>
      </section>

      <section anchor="scope" numbered="true" toc="default">
        <name>Scope and Applicability</name>
        <t>
          This document addresses the benchmarking of Ethernet-based network
          fabrics carrying AI inference serving traffic.
        </t>
        <t>
          The scope covers Layer 2/3 fabric performance (switch forwarding,
          link utilization, congestion management), RDMA transport performance
          (one-sided PUT/GET operations for KV cache transfer, two-sided
          SEND/RECV for expert parallelism dispatch), and the interaction
          between fabric behavior and application-level inference metrics
          (TTFT, ITL, TPS).
        </t>
        <t>
          The document does NOT address benchmarking of individual accelerator
          (GPU/XPU) compute performance, model accuracy or quality metrics,
          benchmarking of the inference serving software stack in isolation
          from the fabric, or live production network testing. All
          methodologies assume controlled laboratory conditions per BMWG
          convention.
        </t>
      </section>

      <section anchor="relationship-bmwg" numbered="true" toc="default">
        <name>Relationship to Existing BMWG Work</name>
        <t>
          This document builds upon the foundational BMWG benchmarking
          framework established by <xref target="RFC1242"/>,
          <xref target="RFC2544"/>, <xref target="RFC2889"/>, and
          <xref target="RFC6349"/>.
        </t>
        <t>
          The test structure follows RFC 2544 conventions for trial duration
          (minimum 60 seconds), statistical repetition (minimum 20 trials for
          latency, 50 for burst), and reporting format (graphical and tabular).
        </t>
        <t>
          The methodologies extend RFC 2544 Section 26 benchmarks (throughput,
          latency, frame loss rate, back-to-back frames, system recovery,
          reset) to inference-specific scenarios including KV cache transfer,
          expert parallelism dispatch, and disaggregated serving request
          routing.
        </t>
      </section>

      <section anchor="relationship-companion" numbered="true" toc="default">
        <name>Relationship to Companion Documents</name>
        <t>
          This document is a companion to
          <xref target="TRAINING-BENCH"/>, which defines benchmarking
          methodologies for AI training network fabrics. Both documents share
          common terminology (Section 2), test topology conventions (Section 3),
          and reporting formats (Section 14). Both documents SHOULD be used in
          conjunction with the terminology document <xref target="TERMINOLOGY"/>,
          which defines the common terminology base for AI fabric benchmarking
          and SHOULD be consulted before applying the methodologies in either
          document.
        </t>
        <t>
          Where training workloads are dominated by bulk synchronous collective
          communication (AllReduce, AllGather) with high bandwidth utilization
          and periodic synchronization barriers, inference workloads are
          dominated by bursty, latency-sensitive point-to-point transfers (KV
          cache) and fine-grained AllToAll dispatch (MoE expert parallelism).
          Implementers deploying converged fabrics that serve both training and
          inference workloads SHOULD run both test suites.
        </t>
      </section>
    </section>

    <section anchor="terminology" numbered="true" toc="default">
      <name>Terminology and Definitions</name>
      <t>
        The following terminology is used throughout this document. Terms
        defined in the companion training document are referenced but not
        redefined unless the inference context introduces substantive
        differences.
      </t>
      <dl newline="false" spacing="normal">
        <dt>TTFT</dt>
        <dd>
          Time to First Token. The elapsed time from receipt of an inference
          request by the serving system to emission of the first output token.
          Includes prompt processing (prefill), KV cache generation, optional
          KV cache transfer (in disaggregated architectures), and initial
          decode step. Target: &lt; 500 ms for interactive serving.
        </dd>
        <dt>ITL</dt>
        <dd>
          Inter-Token Latency. The elapsed time between successive output
          tokens during the autoregressive decode phase. Measured at P50, P95,
          P99, and P99.9 percentiles. Target: &lt; 50 ms P99 for interactive
          serving.
        </dd>
        <dt>TPS</dt>
        <dd>
          Tokens Per Second. Aggregate throughput of the serving system
          measured as the total number of output tokens generated per second
          across all concurrent requests. Reported separately for input
          (prefill) TPS and output (decode) TPS.
        </dd>
        <dt>KV Cache</dt>
        <dd>
          Key-Value Cache. The intermediate attention state (key and value
          projection matrices) computed during the prefill phase and reused
          during each decode step. Size scales with model dimension, number of
          layers, number of attention heads, sequence length, and numerical
          precision. For a 70B parameter model at FP16 with 4K context:
          approximately 1.34 GB per request.
        </dd>
        <dt>Prefill Phase</dt>
        <dd>
          The compute-bound phase of inference in which the entire input
          prompt is processed in parallel to generate the KV cache and the
          first output token. Characterized by high arithmetic intensity
          (200-400 ops/byte), high GPU utilization (90-95%), and large
          activation tensors.
        </dd>
        <dt>Decode Phase</dt>
        <dd>
          The memory-bound phase of inference in which output tokens are
          generated autoregressively, one token per forward pass.
          Characterized by low arithmetic intensity (60-80 ops/byte), lower
          GPU utilization (20-40%), and memory-bandwidth-limited KV cache
          reads.
        </dd>
        <dt>Disaggregated Serving</dt>
        <dd>
          An inference serving architecture <xref target="DISTSERVE"/> in
          which prefill and decode computations are executed on physically
          separate groups of accelerators (workers), connected by a network
          fabric. The KV cache generated by prefill workers are transferred
          over the fabric to decode workers.
        </dd>
        <dt>xPyD Ratio</dt>
        <dd>
          The allocation ratio of prefill (x) to decode (y) resources in a
          disaggregated serving cluster. For example, 3P9D indicates 3 prefill
          nodes and 9 decode nodes. The optimal ratio depends on model size,
          prompt length distribution, output length distribution, and SLO
          targets.
        </dd>
        <dt>EP</dt>
        <dd>
          Expert Parallelism. A parallelism strategy for Mixture-of-Experts
          (MoE) models in which expert sub-networks are distributed across
          multiple GPUs. Token routing to the appropriate experts requires
          AllToAll communication.
        </dd>
        <dt>Wide EP</dt>
        <dd>
          Expert Parallelism spanning many GPUs (e.g., 96-way EP across 12
          nodes), requiring inter-node AllToAll communication for every MoE
          layer forward pass.
        </dd>
        <dt>DP Attention</dt>
        <dd>
          Data Parallelism applied to the attention computation, where the KV
          cache is partitioned across data-parallel ranks. Each rank holds
          1/DP_SIZE of the KV cache, and AllToAll communication is used to
          exchange attention outputs.
        </dd>
        <dt>MoE</dt>
        <dd>
          Mixture of Experts. A model architecture that activates only a
          subset of expert sub-networks for each token, enabling larger model
          capacity with sub-linear compute scaling.
        </dd>
        <dt>Normal Dispatch</dt>
        <dd>
          A communication mode for AllToAll MoE dispatch optimized for the
          prefill phase. Maximizes throughput for long input sequences but
          generates dynamic (symbolic) shapes incompatible with CUDA Graph.
        </dd>
        <dt>Low-Latency Dispatch</dt>
        <dd>
          A communication mode for AllToAll MoE dispatch <xref target="EP-COMM"/>
          optimized for the decode phase. Uses fixed input shapes compatible
          with CUDA Graph, reducing kernel launch overhead at the cost of
          slightly lower peak throughput.
        </dd>
        <dt>RDMA</dt>
        <dd>
          Remote Direct Memory Access. A transport mechanism enabling direct
          memory-to-memory data transfer between hosts without CPU involvement.
          Implementations include InfiniBand Verbs and RoCEv2 (RDMA over
          Converged Ethernet v2).
        </dd>
        <dt>RoCEv2</dt>
        <dd>
          RDMA over Converged Ethernet version 2. An RDMA transport that
          encapsulates InfiniBand transport over UDP/IP, enabling RDMA
          semantics on standard Ethernet fabrics.
        </dd>
        <dt>UET</dt>
        <dd>
          Ultra Ethernet Transport. A transport protocol defined by the Ultra
          Ethernet Consortium (UEC) Specification 1.0 <xref target="UEC-SPEC"/>, offering
          ordered/unordered reliable delivery, multipath packet spraying, and
          integrated congestion control for AI/HPC workloads.
        </dd>
        <dt>KVCXL</dt>
        <dd>
          KV Cache Transfer Library. A library providing standardized
          point-to-point data transfer primitives (register, transfer, notify)
          for inference engines, abstracting underlying transports (intra-node
          interconnect, RDMA, PCIe, and storage interfaces). Multiple
          open-source and vendor implementations exist.
        </dd>
        <dt>GIN</dt>
        <dd>
          GPU-Initiated Networking. A communication paradigm where GPU threads
          directly initiate network operations (RDMA sends, one-sided puts)
          without CPU involvement, reducing latency by eliminating CPU-GPU
          synchronization.
        </dd>
        <dt>PagedAttention</dt>
        <dd>
          A memory management technique for KV caches <xref target="VLLM"/>
          that stores attention keys and values in fixed-size pages (typically,
          16-64 KB), enabling non-contiguous allocation and reducing memory
          fragmentation.
        </dd>
        <dt>Continuous Batching</dt>
        <dd>
          A scheduling technique that dynamically adds new requests to an
          active inference batch as decode slots become available, improving
          GPU utilization compared to static batching.
        </dd>
        <dt>Prefix Caching</dt>
        <dd>
          Reuse of previously computed KV cache segments for prompts that
          share a common prefix (e.g., system prompt), avoiding redundant
          prefill computation.
        </dd>
        <dt>DUT</dt>
        <dd>
          Device Under Test. In this document, the DUT is one or more network
          fabric elements (switches, NICs, or the complete fabric) whose
          performance impact on inference serving is being characterized.
        </dd>
        <dt>SUT</dt>
        <dd>
          System Under Test. The complete inference serving system including
          accelerators, NICs, fabric, and serving software, when end-to-end
          metrics are being measured.
        </dd>
        <dt>RT</dt>
        <dd>
          Router Tester / Traffic Generator. Test equipment capable of
          generating and receiving network traffic at specified rates with
          timestamping accuracy sufficient for the measurements defined herein.
        </dd>
      </dl>
    </section>

    <section anchor="topology" numbered="true" toc="default">
      <name>Test Topology and Architecture</name>

      <section anchor="ref-topologies" numbered="true" toc="default">
        <name>Reference Fabric Topologies</name>
        <t>
          The reference topologies from the companion training document (2-Tier
          Clos, 3-Tier Clos, Rail-Optimized) remain applicable. Inference
          serving introduces additional topology considerations related to
          disaggregated prefill/decode placement and MoE expert distribution.
        </t>

        <section anchor="topo-a" numbered="true" toc="default">
          <name>Topology A: 2-Tier Clos (Leaf-Spine)</name>
          <t>
            Applicable to inference clusters up to approximately 2,048
            accelerators. Prefill and decode worker groups SHOULD be placed on
            separate leaf switches (or separate leaf switch groups) to isolate
            KV cache transfer traffic from decode-to-client response traffic.
            Expert parallelism (EP) traffic within a single MoE dispatch group
            SHOULD be confined to a single leaf switch or a minimal number of
            leaf switches to minimize spine-hop latency.
          </t>
        </section>

        <section anchor="topo-b" numbered="true" toc="default">
          <name>Topology B: 3-Tier Clos (Leaf-Spine-Superspine)</name>
          <t>
            Required for inference clusters exceeding 2,048 accelerators or
            for multi-model serving deployments where different model instances
            occupy different fabric pods. KV cache transfer traffic between
            prefill and decode workers in different pods traverses the
            superspine tier, making superspine bandwidth and latency critical.
          </t>
        </section>

        <section anchor="topo-c" numbered="true" toc="default">
          <name>Topology C: Disaggregated Prefill/Decode Placement</name>
          <t>
            A topology variant specific to inference serving in which prefill
            workers and decode workers are placed in distinct physical locations
            within the fabric, connected by a dedicated KV cache transfer
            network segment. This topology enables independent scaling of
            prefill and decode resources and allows heterogeneous hardware
            (e.g., high-compute GPUs for prefill, high-memory-bandwidth GPUs
            for decode).
          </t>
          <figure anchor="fig-pd-topology">
            <name>Disaggregated Prefill/Decode Inference Topology</name>
            <artwork align="center" name="" type="" alt=""><![CDATA[
          +----------------------+
          |   Request Router     |
          |   (KV-Aware LB)      |
          +--------+-------------+
                   |
      +------------+--------------+
      |                           |
+-----v-------+         +---------v-----+
| Prefill Pool|         |  Decode Pool  |
| (xP workers)|         |  (yD workers) |
| High Compute|         | High Mem BW   |
| TP=8, DP=N/8|         | TP=8, DP=M/8  |
+------+------+         +-------+-------+
       |                        |
       |  KV Cache RDMA Transfer|
       |  (One-sided PUT/Signal) |
       +------------------------+
            ]]></artwork>
          </figure>
        </section>
      </section>

      <section anchor="disagg-topology" numbered="true" toc="default">
        <name>Disaggregated Prefill/Decode Topology</name>
        <t>
          The disaggregated topology separates the inference pipeline into
          physically distinct pools connected by the fabric. The test topology
          MUST include the following components:
        </t>
        <ul spacing="normal">
          <li>
            <t><strong>Prefill Worker Pool:</strong> N_P nodes, each containing
            G accelerators with high-compute capability. These workers execute
            the prefill phase and generate KV cache state. Tensor Parallelism
            (TP) is applied within each node; Data Parallelism (DP) is applied
            across nodes. Each prefill worker communicates with one or more
            decode workers via RDMA-based KV cache transfer.</t>
          </li>
          <li>
            <t><strong>Decode Worker Pool:</strong> N_D nodes, each containing
            G accelerators with high memory bandwidth. These workers receive KV
            cache state from prefill workers and execute the autoregressive
            decode phase. DP Attention may partition the KV cache across DP
            ranks within the decode pool, requiring AllToAll communication
            during decode.</t>
          </li>
          <li>
            <t><strong>KV Cache Transfer Network:</strong> The fabric segment
            connecting prefill and decode pools. This segment carries one-sided
            RDMA PUT operations (or PUT-with-signal) transferring KV cache
            blocks from prefill GPU memory to decode GPU memory. The transfer
            path may traverse NVLink (intra-node), PCIe/CXL (to NIC), and
            Ethernet/InfiniBand (inter-node).</t>
          </li>
          <li>
            <t><strong>Request Router:</strong> A network-layer or
            application-layer load balancer that assigns incoming inference
            requests to prefill workers and subsequently routes KV cache to the
            appropriate decode workers. KV-aware routing and prefix-aware
            caching policies are under test.</t>
          </li>
        </ul>
      </section>

      <section anchor="dut-id" numbered="true" toc="default">
        <name>Device Under Test (DUT) Identification</name>
        <t>
          The following table defines the DUT configurations tested in this
          document. Each DUT configuration represents a different level of
          fabric abstraction.
        </t>
        <table anchor="tab-dut">
          <name>DUT Configuration Definitions</name>
          <thead>
            <tr><th>DUT ID</th><th>Description</th><th>Components Under Test</th></tr>
          </thead>
          <tbody>
            <tr>
              <td>DUT-S</td>
              <td>Single Switch</td>
              <td>Individual leaf or spine switch forwarding inference traffic. Measures per-hop latency, buffer absorption, ECN marking accuracy.</td>
            </tr>
            <tr>
              <td>DUT-F</td>
              <td>Complete Fabric</td>
              <td>End-to-end fabric from prefill NIC egress to decode NIC ingress. Measures fabric-level KV cache transfer latency, throughput, and congestion behavior.</td>
            </tr>
            <tr>
              <td>DUT-N</td>
              <td>NIC Transport</td>
              <td>NIC RDMA transport stack processing KV cache transfer operations. Measures RDMA verb completion latency, one-sided PUT bandwidth, QP scaling.</td>
            </tr>
            <tr>
              <td>DUT-PD</td>
              <td>Prefill-Decode Path</td>
              <td>The complete data path from prefill GPU memory through NIC, fabric, NIC, to decode GPU memory. Measures end-to-end KV cache transfer including NVLink, PCIe, and fabric segments.</td>
            </tr>
            <tr>
              <td>SUT-E</td>
              <td>End-to-End System</td>
              <td>Complete inference serving system including inference serving software, RDMA transfer libraries, fabric, and accelerators. Measures TTFT, ITL, TPS as functions of fabric performance.</td>
            </tr>
          </tbody>
        </table>
      </section>

      <section anchor="traffic-gen" numbered="true" toc="default">
        <name>Traffic Generator and Workload Emulator Requirements</name>
        <t>
          Tests in this document require one or both of the following traffic
          generation modes. The mode used MUST be documented in all test
          reports.
        </t>

        <section anchor="hw-tgen" numbered="true" toc="default">
          <name>Hardware Traffic Generator (RT) - Minimum Requirements</name>
          <t>The hardware traffic generator MUST satisfy all of the following:</t>
          <ol type="a" spacing="normal">
            <li>RDMA traffic generation supporting RoCEv2 and, where tested,
            UET transport; configurable RDMA verb types (one-sided PUT,
            PUT-with-signal, two-sided SEND/RECV).</li>
            <li>Configurable message sizes from 4 KB (minimum KV cache page)
            to 256 MB (large KV cache block).</li>
            <li>Configurable QP counts from 1 QP to a minimum of 256 QPs per
            source-destination port pair.</li>
          </ol>
        </section>

        <section anchor="sw-emulator" numbered="true" toc="default">
          <name>Software Workload Emulator (WE) - Minimum Requirements</name>
          <t>
            A software workload emulator runs on actual accelerators and
            generates realistic inference workloads. The WE MUST support all
            of the following:
          </t>
          <ol type="a" spacing="normal">
            <li>Configurable prompt length distributions: uniform, Zipf, and
            trace-replay modes.</li>
            <li>Configurable output length distributions and configurable
            request arrival rates: Poisson, bursty, and trace-replay.</li>
            <li>Disaggregated prefill/decode execution with actual RDMA-based
            KV cache transfer between prefill and decode worker pools.</li>
            <li>MoE expert parallelism with actual AllToAll dispatch where
            MoE-specific tests (Section 7) are performed.</li>
            <li>Measurement instrumentation providing per-request TTFT and ITL
            with timestamp accuracy &lt;= 1 millisecond.</li>
          </ol>
          <t>
            When a software workload emulator is used, the complete software
            configuration MUST be documented per <xref target="dut-id"/>, as
            framework version, RDMA library version, and GPU driver version
            materially affect results.
          </t>
        </section>
      </section>
    </section>

    <section anchor="kpi-framework" numbered="true" toc="default">
      <name>KPI Framework and Metrics Taxonomy</name>
      <t>
        This section defines the Key Performance Indicators measured across
        all test categories. KPIs are organized into four tiers: Primary
        Latency KPIs (end-user-facing response time metrics), Primary
        Throughput KPIs (system-level capacity metrics), Fabric-Level KPIs
        (network-specific measurements), and Fabric Health Indicators
        (operational monitoring metrics).
      </t>
      <aside>
        <t>
          NOTE: Where numerical reference values appear in the Target column of
          the KPI tables below (including TTFT, ITL, and other latency
          targets), these values are provided as informational reference points
          reflecting current industry observations for interactive inference
          workloads. They do NOT constitute benchmarking acceptance criteria or
          performance requirements. Per the BMWG charter, the definition of
          acceptance criteria or performance requirements is explicitly outside
          the scope of this Working Group. SLO values cited in this document
          are illustrative examples from current industry practice.
          Implementers MAY use these values as contextual references when
          interpreting results; they MUST NOT be used as pass/fail thresholds
          in vendor evaluations. Deployment-specific SLOs will vary by
          application, model architecture, and operator requirements.
        </t>
        <t>
          NOTE: Target values in this section are NON-NORMATIVE illustrative
          reference points derived from current industry practice as of
          2025-2026. They do NOT constitute benchmarking acceptance criteria
          or performance requirements. Per BMWG charter, defining acceptance
          criteria is explicitly out of scope. Implementers MAY use these
          values as contextual references only; they MUST NOT be used as
          pass/fail thresholds.
        </t>
      </aside>

      <section anchor="latency-kpis" numbered="true" toc="default">
        <name>Primary Latency KPIs</name>
        <table anchor="tab-latency-kpis">
          <name>Primary Latency KPIs</name>
          <thead>
            <tr><th>KPI</th><th>Unit</th><th>Definition</th><th>Target (Interactive)</th><th>Measurement Point</th></tr>
          </thead>
          <tbody>
            <tr>
              <td>TTFT</td><td>ms</td>
              <td>Time from request arrival to first output token emission</td>
              <td>&lt; 500 ms P99</td><td>SUT-E request/response boundary</td>
            </tr>
            <tr>
              <td>ITL</td><td>ms</td>
              <td>Time between successive output tokens</td>
              <td>&lt; 50 ms P99</td><td>SUT-E token emission timestamps</td>
            </tr>
            <tr>
              <td>TTFT_fabric</td><td>ms</td>
              <td>Fabric contribution to TTFT (KV cache transfer latency)</td>
              <td>&lt; 300 ms P99</td><td>DUT-PD NIC-to-NIC measurement</td>
            </tr>
            <tr>
              <td>ITL_fabric</td><td>ms</td>
              <td>Fabric contribution to ITL (EP dispatch latency per decode step)</td>
              <td>&lt; 5 ms P99</td><td>DUT-F EP dispatch round-trip</td>
            </tr>
            <tr>
              <td>E2E_latency</td><td>ms</td>
              <td>End-to-end request latency from arrival to completion of all output tokens</td>
              <td>Varies by output length</td><td>SUT-E request/response boundary</td>
            </tr>
          </tbody>
        </table>
      </section>

      <section anchor="throughput-kpis" numbered="true" toc="default">
        <name>Primary Throughput KPIs</name>
        <table anchor="tab-throughput-kpis">
          <name>Primary Throughput KPIs</name>
          <thead>
            <tr><th>KPI</th><th>Unit</th><th>Definition</th><th>Measurement Point</th></tr>
          </thead>
          <tbody>
            <tr>
              <td>TPS_input</td><td>tokens/s</td>
              <td>Aggregate input (prefill) tokens processed per second across all workers</td>
              <td>SUT-E prefill completion events</td>
            </tr>
            <tr>
              <td>TPS_output</td><td>tokens/s</td>
              <td>Aggregate output (decode) tokens generated per second across all workers</td>
              <td>SUT-E token emission events</td>
            </tr>
            <tr>
              <td>TPS_per_GPU</td><td>tokens/s/GPU</td>
              <td>Output tokens per second normalized by number of decode GPUs</td>
              <td>SUT-E per-worker counters</td>
            </tr>
            <tr>
              <td>Goodput</td><td>tokens/s</td>
              <td>Effective output token rate excluding tokens from preempted, evicted, or failed requests</td>
              <td>SUT-E successful completion events</td>
            </tr>
            <tr>
              <td>KV_BW</td><td>GB/s</td>
              <td>Aggregate KV cache transfer bandwidth between prefill and decode pools</td>
              <td>DUT-PD RDMA counters</td>
            </tr>
            <tr>
              <td>Request_Rate</td><td>req/s</td>
              <td>Maximum sustained request arrival rate meeting all latency SLOs</td>
              <td>SUT-E admission control boundary</td>
            </tr>
          </tbody>
        </table>
      </section>

      <section anchor="fabric-kpis" numbered="true" toc="default">
        <name>Fabric-Level KPIs</name>
        <table anchor="tab-fabric-kpis">
          <name>Fabric-Level KPIs</name>
          <thead>
            <tr><th>KPI</th><th>Unit</th><th>Definition</th><th>DUT</th></tr>
          </thead>
          <tbody>
            <tr><td>KV_xfer_latency</td><td>us</td><td>One-sided RDMA PUT completion time for a single KV cache block transfer</td><td>DUT-N</td></tr>
            <tr><td>KV_xfer_bandwidth</td><td>GB/s</td><td>Sustained unidirectional KV cache transfer throughput per NIC port</td><td>DUT-N</td></tr>
            <tr><td>EP_alltoall_latency</td><td>us</td><td>Round-trip time for a complete MoE expert parallelism AllToAll dispatch</td><td>DUT-F</td></tr>
            <tr><td>EP_alltoall_bandwidth</td><td>GB/s</td><td>Aggregate AllToAll bandwidth across all EP ranks during dispatch</td><td>DUT-F</td></tr>
            <tr><td>Fabric_FCT</td><td>us</td><td>Flow completion time for a KV cache transfer flow through the fabric</td><td>DUT-F</td></tr>
            <tr><td>Buffer_utilization</td><td>%</td><td>Peak switch buffer utilization during KV cache transfer bursts</td><td>DUT-S</td></tr>
            <tr><td>ECN_marking_rate</td><td>%</td><td>Fraction of packets marked with ECN-CE during inference traffic</td><td>DUT-S</td></tr>
            <tr><td>PFC_frame_count</td><td>frames</td><td>Number of PFC PAUSE frames generated per unit time</td><td>DUT-S</td></tr>
            <tr><td>Link_utilization</td><td>%</td><td>Average and peak link utilization on fabric links carrying inference traffic</td><td>DUT-F</td></tr>
            <tr><td>Packet_drop_rate</td><td>ppm</td><td>Packets dropped per million due to buffer overflow or transport error</td><td>DUT-F</td></tr>
          </tbody>
        </table>
      </section>

      <section anchor="fabric-health" numbered="true" toc="default">
        <name>Fabric Health Indicators</name>
        <table anchor="tab-health">
          <name>Fabric Health Indicators</name>
          <thead>
            <tr><th>Indicator</th><th>Threshold</th><th>Description</th></tr>
          </thead>
          <tbody>
            <tr><td>CPU Utilization (switch)</td><td>&lt; 30%</td><td>Control plane CPU usage on switches under inference traffic load</td></tr>
            <tr><td>Memory Usage (switch)</td><td>&lt; 70%</td><td>TCAM, buffer, and control plane memory usage</td></tr>
            <tr><td>FEC Error Rate</td><td>&lt; 1e-12 post-FEC BER</td><td>Forward Error Correction effectiveness on fabric links</td></tr>
            <tr><td>CRC Error Count</td><td>0</td><td>Layer 2 CRC errors on any fabric link</td></tr>
            <tr><td>BGP/OSPF Stability</td><td>0 flaps</td><td>Routing protocol adjacency stability under inference load</td></tr>
            <tr><td>NIC QP State</td><td>100% active</td><td>All RDMA Queue Pairs in active state (no error/reset)</td></tr>
            <tr><td>GPU-NIC PCIe BW</td><td>&gt; 90% of theoretical</td><td>PCIe Gen5 x16 bandwidth utilization between GPU and NIC</td></tr>
          </tbody>
        </table>
      </section>
    </section>

    <section anchor="test-cat1" numbered="true" toc="default">
      <name>Test Category 1: RDMA KV Cache Transfer Benchmarks</name>
      <t>
        KV cache transfer between disaggregated prefill and decode workers is
        the defining fabric workload for inference serving. Unlike training
        collectives (AllReduce, AllGather) which are periodic and predictable,
        KV cache transfers are event-driven (triggered by prefill completion)
        and bursty. This section defines benchmarks for characterizing fabric
        performance under KV cache transfer workloads.
      </t>

      <section anchor="test-5-1" numbered="true" toc="default">
        <name>Point-to-Point KV Cache Transfer Throughput</name>
        <t><strong>Objective:</strong> To determine the maximum sustained KV cache transfer throughput between a single prefill worker NIC and a single decode worker NIC across the DUT fabric.</t>
        <t><strong>Procedure:</strong> Configure a single RDMA connection (QP) between the prefill and decode endpoints. Send a sequence of one-sided RDMA PUT operations with message sizes corresponding to KV cache block sizes. The message size sequence MUST include: 64 KB (single attention page), 256 KB, 1 MB, 4 MB, 16 MB, 64 MB, 256 MB (large prompt KV cache), and 1 GB. For each message size, transmit at the maximum rate sustainable by the NIC for a minimum of 60 seconds per trial. Record the achieved throughput in GB/s. Repeat for 1, 4, 8, 16, 32, 64, and 128 concurrent QPs. The DUT is the fabric path from NIC to NIC.</t>
        <t><strong>Measurement:</strong> Record throughput (GB/s), CPU utilization on both endpoints, GPU memory-to-NIC transfer overhead, and NIC hardware offload utilization. The test MUST be repeated a minimum of 20 times per configuration and the average reported.</t>
        <t><strong>Reporting Format:</strong> Results SHOULD be reported as a multi-line graph with message size (log scale) on the X axis and throughput (GB/s) on the Y axis. Separate lines for each QP count. A reference line showing theoretical NIC line rate MUST be included.</t>
      </section>

      <section anchor="test-5-2" numbered="true" toc="default">
        <name>KV Cache Transfer Latency</name>
        <t><strong>Objective:</strong> To determine the latency of individual KV cache block transfers across the DUT fabric under varying load conditions.</t>
        <t><strong>Procedure:</strong> Using the same endpoint configuration as Test 5.1, measure the completion time of individual RDMA PUT operations. Latency is measured from the initiation of the PUT verb on the prefill NIC to receipt of the completion signal on the decode NIC (for PUT-with-signal) or to polling of the remote completion queue. Measure latency under unloaded conditions (single outstanding operation) and under loaded conditions (background traffic at 25%, 50%, 75%, and 90% of fabric capacity). Message sizes MUST include 64 KB, 1 MB, 16 MB, and 256 MB.</t>
        <t><strong>Measurement:</strong> Report latency at P50, P95, P99, and P99.9 percentiles. The test MUST be repeated a minimum of 20 trials of at least 120 seconds each per configuration. The difference between P99 and P50 (tail latency spread) SHOULD be reported as a derived metric.</t>
        <t><strong>Reporting Format:</strong> Results SHOULD be reported as a table with columns for message size, background load level, and latency at each percentile. A complementary CDF plot of latency distribution for selected configurations SHOULD be included.</t>
      </section>

      <section anchor="test-5-3" numbered="true" toc="default">
        <name>Concurrent KV Cache Transfer Scaling</name>
        <t><strong>Objective:</strong> To characterize how aggregate KV cache transfer performance scales as the number of concurrent prefill-to-decode transfer pairs increases.</t>
        <t><strong>Procedure:</strong> Configure N concurrent prefill-decode endpoint pairs, where N ranges from 1 to the maximum supported by the fabric (e.g., 1, 2, 4, 8, 16, 32, 64, 128 pairs). Each pair executes continuous KV cache transfers of 16 MB messages (representative of a medium-length prompt). Measure aggregate throughput and per-pair latency as N increases. The DUT is the complete fabric.</t>
        <t><strong>Measurement:</strong> Report aggregate throughput (GB/s), per-pair median latency (us), per-pair P99 latency (us), Jain Fairness Index across pairs, and maximum fabric link utilization observed. The test MUST be repeated a minimum of 20 times per value of N.</t>
        <t><strong>Reporting Format:</strong> Results SHOULD be reported as a dual-axis graph with N (concurrent pairs) on the X axis, aggregate throughput on the left Y axis, and P99 latency on the right Y axis. The JFI value for each N SHOULD be annotated.</t>
      </section>

      <section anchor="test-5-4" numbered="true" toc="default">
        <name>Multi-Tier Storage Transfer Characterization</name>
        <t><strong>Objective:</strong> To characterize KV cache transfer performance across the memory/storage hierarchy <xref target="LMCACHE"/>: GPU HBM to GPU HBM (inter-node RDMA), GPU HBM to remote CPU DRAM (offload), CPU DRAM to GPU HBM (reload), and GPU HBM to NVMe/SSD (persistent cache).</t>
        <t><strong>Procedure:</strong> For each tier pair, measure unidirectional transfer throughput and latency for message sizes of 1 MB, 16 MB, and 256 MB. Use zero-copy transfers where supported (GDS for NVMe, GPUDirect RDMA for inter-node). The DUT encompasses the complete transfer path including PCIe, NVLink, and fabric segments.</t>
        <t><strong>Measurement:</strong> Report throughput (GB/s) and latency (P50, P99) for each tier pair and message size. Report the tier throughput ratio relative to GPU-to-GPU RDMA as a derived metric.</t>
        <t><strong>Reporting Format:</strong> Results SHOULD be reported as a table with rows for each tier pair and columns for throughput and latency at each message size.</t>
      </section>
    </section>

    <section anchor="test-cat2" numbered="true" toc="default">
      <name>Test Category 2: Prefill/Decode Disaggregation Benchmarks</name>
      <t>
        Disaggregated prefill/decode serving separates the two phases onto
        distinct hardware pools to enable independent optimization and scaling.
        This section benchmarks the fabric's ability to support the resulting
        KV cache transfer traffic patterns and their impact on end-to-end
        inference metrics.
      </t>

      <section anchor="test-6-1" numbered="true" toc="default">
        <name>End-to-End Disaggregated TTFT</name>
        <t><strong>Objective:</strong> To measure TTFT as a function of prompt length in a disaggregated serving configuration, isolating the fabric contribution.</t>
        <t><strong>Procedure:</strong> Configure a disaggregated serving system (SUT-E) with a specified xPyD ratio (e.g., 3P9D for a 12-node cluster). Submit inference requests with prompt lengths of 128, 512, 1024, 2048, 4096, 8192, and 16384 tokens. For each prompt length, measure the total TTFT and decompose it into: T_prefill (prefill compute time), T_transfer (KV cache fabric transfer time, measured at DUT-PD), and T_decode_init (first decode step time). The fabric contribution (T_transfer) is the primary measurement of interest.</t>
        <t><strong>Measurement:</strong> Report TTFT (ms) and its decomposition at P50, P95, and P99 percentiles. The ratio T_transfer/TTFT (fabric fraction) SHOULD be reported as a derived metric. The test MUST be repeated a minimum of 20 trials per prompt length.</t>
        <t><strong>Reporting Format:</strong> Results SHOULD be reported as a stacked bar chart with prompt length on the X axis and TTFT (ms) on the Y axis, with bars decomposed into T_prefill, T_transfer, and T_decode_init. A table of numerical values MUST accompany the chart.</t>
      </section>

      <section anchor="test-6-2" numbered="true" toc="default">
        <name>xPyD Ratio Optimization</name>
        <t><strong>Objective:</strong> To determine the optimal prefill-to-decode resource ratio for a given model, prompt distribution, and latency SLO, as limited by fabric transfer capacity.</t>
        <t><strong>Procedure:</strong> For a fixed total number of nodes N (e.g., 12), iterate over xPyD ratios: 1P11D, 2P10D, 3P9D, 4P8D, 6P6D, 8P4D, 10P2D, 11P1D. For each ratio, submit a sustained request stream matching a target request rate (e.g., 10, 50, 100, 200 req/s) with a specified prompt length distribution (e.g., Zipf with alpha=1.0 over [128, 8192] tokens). Measure TTFT P99, ITL P99, TPS_output, and Goodput for each configuration.</t>
        <t><strong>Measurement:</strong> Report all four metrics for each xPyD ratio and request rate. Identify the Pareto-optimal ratio(s) that maximize TPS_output while meeting TTFT P99 &lt; 500 ms and ITL P99 &lt; 50 ms.</t>
        <t><strong>Reporting Format:</strong> Results SHOULD be reported as a multi-panel figure with one panel per request rate, each showing xPyD ratio on the X axis and metrics on dual Y axes (TTFT/ITL on left, TPS on right). The Pareto frontier SHOULD be highlighted.</t>
      </section>

      <section anchor="test-6-3" numbered="true" toc="default">
        <name>Heterogeneous Parallelism Configuration</name>
        <t><strong>Objective:</strong> To evaluate the fabric impact of using different parallelism strategies on prefill vs. decode pools in a disaggregated configuration.</t>
        <t><strong>Procedure:</strong> Test the following parallelism configurations: (a) Prefill TP=8, Decode TP=8 (baseline), (b) Prefill TP=8, Decode TP=4 with DP_Attention=2, (c) Prefill TP=4 with DP=2, Decode TP=2 with DP_Attention=4. For each configuration, measure KV cache transfer patterns (number of flows, message sizes, concurrency), fabric bandwidth consumed, and end-to-end TTFT/ITL.</t>
        <t><strong>Measurement:</strong> Report the number of concurrent RDMA flows, aggregate bandwidth (GB/s), TTFT (ms), and ITL (ms) at P50 and P99 for each configuration.</t>
      </section>

      <section anchor="test-6-4" numbered="true" toc="default">
        <name>Prefill Queue Depth Impact on Transfer Latency</name>
        <t><strong>Objective:</strong> To measure how queuing of prefill requests (due to compute contention) affects KV cache transfer burstiness and fabric congestion.</t>
        <t><strong>Procedure:</strong> Oversubscribe the prefill pool by submitting requests at a rate exceeding prefill capacity. Measure the resulting KV cache transfer burst characteristics: burst size (number of concurrent transfers), burst duration, inter-burst gap, and peak fabric bandwidth demand. Vary the oversubscription ratio from 1.0x (saturated) to 2.0x in 0.25x increments.</t>
        <t><strong>Measurement:</strong> Report burst size distribution, peak and average fabric bandwidth, KV transfer latency P99, and ECN/PFC event counts as functions of oversubscription ratio.</t>
      </section>
    </section>

    <section anchor="test-cat3" numbered="true" toc="default">
      <name>Test Category 3: MoE Expert Parallelism Benchmarks</name>
      <t>
        Mixture-of-Experts models distribute expert sub-networks across GPUs
        and route tokens to the appropriate experts via AllToAll communication.
        This section benchmarks the fabric's ability to support the resulting
        fine-grained, latency-sensitive inter-GPU traffic patterns.
      </t>

      <section anchor="test-7-1" numbered="true" toc="default">
        <name>AllToAll Dispatch Throughput</name>
        <t><strong>Objective:</strong> To determine the maximum AllToAll dispatch throughput for MoE expert parallelism across the DUT fabric.</t>
        <t><strong>Procedure:</strong> Configure N GPUs in an EP group (e.g., N = 8, 16, 32, 64, 96). Each GPU holds M/N experts where M is the total expert count. Generate a synthetic MoE dispatch workload where each GPU sends token embeddings to the experts selected by a top-k routing function (k=2 typical). Measure the aggregate AllToAll bandwidth and per-dispatch latency for batch sizes of 1, 8, 32, 128, and 512 tokens, and EP group sizes of 8, 16, 32, 64, and 96.</t>
        <t><strong>Measurement:</strong> Report aggregate bandwidth (GB/s), per-dispatch latency (us) at P50 and P99, and GPU idle time waiting for dispatch completion. The test MUST be repeated a minimum of 20 times per configuration.</t>
        <t><strong>Reporting Format:</strong> Results SHOULD be reported as a heatmap with EP group size on the Y axis, batch size on the X axis, and throughput (GB/s) as the color dimension. A companion latency table MUST be included.</t>
      </section>

      <section anchor="test-7-2" numbered="true" toc="default">
        <name>Normal vs. Low-Latency Dispatch Mode Comparison</name>
        <t><strong>Objective:</strong> To compare fabric performance under Normal Dispatch (optimized for prefill, dynamic shapes, incompatible with CUDA Graph) and Low-Latency Dispatch (optimized for decode, fixed shapes, CUDA Graph compatible).</t>
        <t><strong>Procedure:</strong> Execute identical MoE dispatch workloads in both modes for EP group sizes of 8, 32, and 96 GPUs. For Normal Dispatch, use batch sizes of 128, 256, and 512 (typical prefill). For Low-Latency Dispatch, use batch sizes of 1, 4, 8, and 16 (typical decode). Measure dispatch latency, fabric bandwidth, and the impact of CUDA Graph on kernel launch overhead.</t>
        <t><strong>Measurement:</strong> Report per-dispatch latency (us) at P50, P95, P99 for each mode. Report the latency ratio LL/Normal for equivalent EP sizes. Report the fabric bandwidth utilization (%) for each mode.</t>
      </section>

      <section anchor="test-7-3" numbered="true" toc="default">
        <name>Wide Expert Parallelism Scaling</name>
        <t><strong>Objective:</strong> To characterize AllToAll dispatch performance as EP group size scales beyond a single node (wide EP), requiring inter-node fabric communication.</t>
        <t><strong>Procedure:</strong> Scale the EP group from intra-node only (EP=8 on a single 8-GPU node) to wide EP (EP=16, 32, 48, 64, 96 spanning 2, 4, 6, 8, 12 nodes). For each EP size, measure: intra-node dispatch bandwidth (NVLink), inter-node dispatch bandwidth (RDMA/fabric), total dispatch latency, and the ratio of inter-node to intra-node bandwidth. Use a fixed batch size of 128 tokens and a representative MoE model configuration (256 experts, top-2 routing).</t>
        <t><strong>Measurement:</strong> Report total dispatch latency (us), inter-node bandwidth (GB/s), and latency decomposition (intra-node vs. inter-node fraction). Report the scaling efficiency: (EP=8 latency) / (EP=N latency) * (N/8).</t>
      </section>

      <section anchor="test-7-4" numbered="true" toc="default">
        <name>Expert Parallelism and KV Cache Transfer Contention</name>
        <t><strong>Objective:</strong> To measure the mutual interference between EP AllToAll dispatch traffic and KV cache transfer traffic when both share the same fabric links.</t>
        <t><strong>Procedure:</strong> On a shared fabric, simultaneously execute: (a) continuous KV cache transfers at a sustained rate (e.g., 50%, 75% of fabric capacity), and (b) periodic EP AllToAll dispatches (one per MoE layer forward pass). Measure the impact of each traffic class on the other.</t>
        <t><strong>Measurement:</strong> Report KV_xfer_latency P99 (us) and EP_alltoall_latency P99 (us) for the isolated and contended cases. Report the contention penalty as the ratio of contended P99 to isolated P99 for each traffic class. Report ECN/PFC event counts during contention.</t>
      </section>
    </section>

    <section anchor="test-cat4" numbered="true" toc="default">
      <name>Test Category 4: Congestion Management Benchmarks</name>
      <t>
        Inference traffic patterns differ from training in their burstiness,
        heterogeneity (mixed KV cache transfers and EP dispatches), and
        latency sensitivity. This section adapts the congestion management
        benchmarks from the companion training document to inference-specific
        traffic profiles.
      </t>

      <section anchor="test-8-1" numbered="true" toc="default">
        <name>ECN Marking Under Inference Incast</name>
        <t><strong>Objective:</strong> To verify that ECN marking thresholds are correctly applied when multiple prefill workers simultaneously transfer KV cache blocks to a single decode worker (incast pattern).</t>
        <t><strong>Procedure:</strong> Configure M prefill workers (M = 2, 4, 8, 16, 32) to simultaneously transfer 16 MB KV cache blocks to a single decode worker port. Measure the ECN-CE marking rate at the leaf switch connected to the decode worker. Repeat for ECN marking thresholds of 100 KB, 500 KB, 1 MB, and 5 MB. The DUT is the individual leaf switch (DUT-S).</t>
        <t><strong>Measurement:</strong> Report the ECN marking rate (fraction of marked packets), the onset of marking (time from first transfer to first ECN mark), queue depth at marking onset, and aggregate throughput achieved. Repeat a minimum of 20 times per configuration.</t>
      </section>

      <section anchor="test-8-2" numbered="true" toc="default">
        <name>PFC Behavior Under Bursty KV Cache Transfers</name>
        <t><strong>Objective:</strong> To characterize PFC PAUSE frame generation and propagation under bursty KV cache transfer patterns typical of disaggregated serving.</t>
        <t><strong>Procedure:</strong> Generate KV cache transfer bursts matching the following profile: N_burst concurrent transfers (N_burst = 4, 8, 16, 32), each of size S_block (16 MB), arriving within a window of T_arrival (100 us, 1 ms, 10 ms). Vary the PFC threshold from 10 KB to 1 MB.</t>
        <t><strong>Measurement:</strong> Report PFC frame count, total PAUSE duration (us), head-of-line blocking delay imposed on other traffic classes (us), and KV cache transfer completion time.</t>
      </section>

      <section anchor="test-8-3" numbered="true" toc="default">
        <name>Congestion Control Convergence for Mixed Traffic</name>
        <t><strong>Objective:</strong> To measure the convergence time of DCQCN (or UET congestion control) when KV cache transfer traffic and EP AllToAll dispatch traffic share fabric capacity.</t>
        <t><strong>Procedure:</strong> Establish a sustained KV cache transfer at 80% of fabric capacity. Introduce EP AllToAll dispatch traffic on the same fabric links. Measure the time for the congestion control algorithm to converge to a stable rate allocation between the two traffic classes. Repeat the test with the roles reversed (EP traffic established first, KV cache introduced).</t>
        <t><strong>Measurement:</strong> Report convergence time (ms) to within 5% of steady-state rates, steady-state bandwidth allocation between traffic classes, packet loss during convergence, and Jain Fairness Index of the steady-state allocation.</t>
      </section>

      <section anchor="test-8-4" numbered="true" toc="default">
        <name>PFC Storm and Deadlock Resilience</name>
        <t><strong>Objective:</strong> To verify that the fabric does not enter a PFC storm or deadlock condition under adversarial inference traffic patterns.</t>
        <t><strong>Procedure:</strong> Per the companion training document, generate a PFC storm scenario by creating circular buffer dependency across multiple switches. Simultaneously inject KV cache transfer traffic on all affected paths. Monitor for: PFC storm propagation, deadlock (zero throughput on any link for &gt; 100 ms), and recovery time. The test duration MUST be at least 300 seconds.</t>
        <t><strong>Measurement:</strong> Report whether PFC storm occurred (yes/no), deadlock occurred (yes/no), maximum PAUSE propagation depth (number of hops), maximum zero-throughput duration (ms), and recovery time (ms).</t>
      </section>
    </section>

    <section anchor="test-cat5" numbered="true" toc="default">
      <name>Test Category 5: Request Routing and Load Balancing</name>
      <t>
        Inference serving introduces application-layer routing decisions that
        interact with fabric-layer load balancing (ECMP, flowlet, packet
        spray). This section benchmarks both layers.
      </t>

      <section anchor="test-9-1" numbered="true" toc="default">
        <name>KV-Aware Request Routing Efficacy</name>
        <t><strong>Objective:</strong> To measure the effectiveness of KV-aware request routing, where the request router considers decode worker KV cache memory occupancy and fabric path congestion when assigning requests.</t>
        <t><strong>Procedure:</strong> Configure a request router with KV-aware routing enabled. Submit a sustained request stream at rates of 10, 50, 100, and 200 req/s with varying prompt lengths. Compare against round-robin routing (baseline).</t>
        <t><strong>Measurement:</strong> Report the coefficient of variation (CV) of decode worker memory utilization, P99 TTFT, P99 ITL, KV cache eviction rate, and Goodput for both KV-aware and round-robin routing.</t>
      </section>

      <section anchor="test-9-2" numbered="true" toc="default">
        <name>Prefix-Aware Cache Hit Rate</name>
        <t><strong>Objective:</strong> To measure the fabric bandwidth savings achieved by prefix-aware caching, where requests with common prefixes are routed to workers that already hold the corresponding KV cache segment.</t>
        <t><strong>Procedure:</strong> Generate a request workload where P% of requests share a common prefix of L tokens (P = 25%, 50%, 75%, 90%; L = 256, 512, 1024, 2048). Measure the prefix cache hit rate, the resulting reduction in KV cache transfer bandwidth, and the impact on TTFT. Compare against non-prefix-aware routing.</t>
        <t><strong>Measurement:</strong> Report cache hit rate (%), fabric bandwidth reduction (%), TTFT reduction (ms), and TPS improvement (%) for each (P, L) combination.</t>
      </section>

      <section anchor="test-9-3" numbered="true" toc="default">
        <name>ECMP and Dynamic Load Balancing Under Inference Traffic</name>
        <t><strong>Objective:</strong> To evaluate fabric-layer load balancing (ECMP, flowlet-based DLB, packet spray) effectiveness under inference traffic patterns characterized by a mix of large KV cache flows and small EP dispatch flows.</t>
        <t><strong>Procedure:</strong> Measure link utilization uniformity under: (a) KV cache transfers only (large flows, 16 MB+), (b) EP AllToAll dispatches only (small flows, &lt; 1 MB), (c) mixed KV cache and EP traffic.</t>
        <t><strong>Measurement:</strong> Report JFI, maximum link utilization (%), minimum link utilization (%), and the oversubscription ratio for each scenario and load balancing algorithm.</t>
      </section>

      <section anchor="test-9-4" numbered="true" toc="default">
        <name>Jain Fairness Index for Decode Worker Utilization</name>
        <t><strong>Objective:</strong> To measure how evenly the fabric distributes KV cache transfer load across decode workers, which directly impacts decode worker GPU utilization balance.</t>
        <t><strong>Procedure:</strong> With N_D decode workers (N_D = 8, 16, 32, 64), submit a sustained request stream and measure the per-worker KV cache receive rate (GB/s), per-worker GPU utilization (%), and per-worker output TPS. Calculate the Jain Fairness Index across decode workers for each metric.</t>
        <t><strong>Measurement:</strong> Report JFI for KV cache receive rate, GPU utilization, and output TPS. Report the max/min ratio for each metric.</t>
      </section>
    </section>

    <section anchor="test-cat6" numbered="true" toc="default">
      <name>Test Category 6: Latency Benchmarks</name>
      <t>
        Inference latency is the primary user-facing quality metric. This
        section defines benchmarks that isolate the fabric's contribution to
        end-to-end inference latency under various operating conditions.
      </t>

      <section anchor="test-10-1" numbered="true" toc="default">
        <name>TTFT Under Varying Prompt Lengths</name>
        <t><strong>Objective:</strong> To characterize TTFT as a function of prompt length, isolating the fabric-dependent KV cache transfer component.</t>
        <t><strong>Procedure:</strong> Submit single requests (no concurrent load) with prompt lengths of 128, 256, 512, 1024, 2048, 4096, 8192, and 16384 tokens. For each prompt length, measure TTFT and decompose into T_prefill, T_transfer, and T_decode_init. The KV cache size scales linearly with prompt length; the expected transfer size for a 70B model at FP16 is approximately 0.33 MB per token.</t>
        <t><strong>Measurement:</strong> Report TTFT, T_transfer, and T_transfer/TTFT at P50, P95, P99 for each prompt length. The test MUST be repeated a minimum of 100 times per prompt length to obtain stable percentile estimates.</t>
        <t><strong>Reporting Format:</strong> Results SHOULD be reported as a line graph with prompt length (tokens) on the X axis and TTFT (ms) on the Y axis, with separate lines for P50 and P99. The T_transfer component SHOULD be shown as a shaded region.</t>
      </section>

      <section anchor="test-10-2" numbered="true" toc="default">
        <name>ITL Characterization and Tail Latency</name>
        <t><strong>Objective:</strong> To characterize inter-token latency distribution and identify fabric-induced tail latency during the decode phase.</t>
        <t><strong>Procedure:</strong> Submit a single long-output request (e.g., 2048 output tokens) and record the timestamp of each emitted token. Repeat under: (a) unloaded fabric, (b) loaded fabric (50% of capacity), and (c) heavily loaded fabric (90% of capacity plus concurrent EP dispatches).</t>
        <t><strong>Measurement:</strong> Report ITL at P50, P95, P99, P99.9, and maximum for each load condition. Report the number of tokens exhibiting ITL &gt; 100 ms (stall events). The test MUST generate at least 10,000 ITL samples per condition.</t>
      </section>

      <section anchor="test-10-3" numbered="true" toc="default">
        <name>End-to-End Latency Under Multi-Tenant Load</name>
        <t><strong>Objective:</strong> To measure inference latency when multiple models or model instances share the same fabric, including EVPN-segmented deployments per <xref target="RFC7432"/>.</t>
        <t><strong>Procedure:</strong> Deploy two or more model instances on separate worker pools sharing the same fabric. Submit requests to both instances concurrently at specified rates. Measure TTFT and ITL for each instance independently.</t>
        <t><strong>Measurement:</strong> Report per-instance TTFT P99, ITL P99, and the interference penalty: (multi-tenant metric - single-tenant metric) / single-tenant metric * 100%.</t>
      </section>

      <section anchor="test-10-4" numbered="true" toc="default">
        <name>Latency Sensitivity to Fabric Congestion</name>
        <t><strong>Objective:</strong> To establish the relationship between fabric congestion level and inference latency degradation.</t>
        <t><strong>Procedure:</strong> Inject controlled background traffic on the fabric at levels from 0% to 95% of capacity in 5% increments. At each level, submit inference requests at a fixed rate and measure TTFT and ITL.</t>
        <t><strong>Measurement:</strong> Report TTFT P99 and ITL P99 as functions of background traffic level. Identify the inflection point at which latency begins to degrade significantly. Report the latency degradation factor at 50%, 75%, and 90% background load.</t>
      </section>
    </section>

    <section anchor="test-cat7" numbered="true" toc="default">
      <name>Test Category 7: Throughput Benchmarks</name>
      <t>
        Inference throughput determines the cost-effectiveness of the serving
        deployment. This section benchmarks the fabric's impact on aggregate
        token generation throughput.
      </t>

      <section anchor="test-11-1" numbered="true" toc="default">
        <name>Aggregate Tokens Per Second</name>
        <t><strong>Objective:</strong> To determine the maximum sustained aggregate TPS achievable while meeting latency SLOs, as a function of fabric configuration.</t>
        <t><strong>Procedure:</strong> Increase the request arrival rate from 1 req/s to the point where either TTFT P99 exceeds 500 ms or ITL P99 exceeds 50 ms. At each rate, measure TPS_output, TPS_input, Goodput, and all latency KPIs. The maximum rate at which all SLOs are met defines the SLO-bounded throughput.</t>
        <t><strong>Measurement:</strong> Report TPS_output, TPS_input, Goodput, TTFT P99, ITL P99, and fabric utilization at the SLO-bounded throughput. Report the fabric utilization at the SLO boundary as a key efficiency metric.</t>
      </section>

      <section anchor="test-11-2" numbered="true" toc="default">
        <name>Batch Size Scaling and Continuous Batching Impact</name>
        <t><strong>Objective:</strong> To measure the interaction between inference batch size, continuous batching <xref target="SGLANG"/>, and fabric transfer patterns.</t>
        <t><strong>Procedure:</strong> Configure the serving system with varying maximum batch sizes (1, 4, 8, 16, 32, 64, 128). For each batch size, measure: (a) the number of concurrent KV cache transfers, (b) aggregate fabric bandwidth consumed, (c) TPS_output, and (d) TTFT P99. Enable continuous batching and repeat.</t>
        <t><strong>Measurement:</strong> Report TPS_output, TTFT P99, fabric bandwidth (GB/s), and peak concurrent transfers for each batch size, with and without continuous batching.</t>
      </section>

      <section anchor="test-11-3" numbered="true" toc="default">
        <name>Goodput Under Preemption and Eviction</name>
        <t><strong>Objective:</strong> To measure the Goodput loss when fabric congestion forces KV cache eviction or request preemption.</t>
        <t><strong>Procedure:</strong> Oversubscribe the system beyond the SLO-bounded throughput (at 110%, 125%, 150%, and 200% of the rate found in Test 11.1). Measure the rate of KV cache evictions, request preemptions, and the resulting Goodput reduction.</t>
        <t><strong>Measurement:</strong> Report Goodput, eviction rate (evictions/s), preemption rate (preemptions/s), wasted fabric bandwidth (GB/s transferred for ultimately evicted KV caches), and the Goodput/TPS_output ratio (efficiency).</t>
      </section>
    </section>

    <section anchor="test-cat8" numbered="true" toc="default">
      <name>Test Category 8: Scale and Autoscaling</name>
      <t>
        Inference serving clusters must scale dynamically to match request
        demand. This section benchmarks the fabric's behavior under scale-up,
        scale-down, and link failure conditions.
      </t>

      <section anchor="test-12-1" numbered="true" toc="default">
        <name>Fabric Scale Limits for Inference Clusters</name>
        <t><strong>Objective:</strong> To determine the maximum inference cluster size supportable by the DUT fabric while meeting performance requirements.</t>
        <t><strong>Procedure:</strong> Progressively scale the cluster from a minimal configuration (e.g., 2 nodes, 16 GPUs) to the fabric's capacity (e.g., 1024 nodes, 8192 GPUs). At each scale point, measure KV cache transfer throughput and latency, EP AllToAll dispatch latency, fabric control plane convergence time, routing table size, and end-to-end TTFT and TPS. The scaling sequence SHOULD follow powers of two.</t>
        <t><strong>Measurement:</strong> Report all KPIs at each scale point. Identify the scale limit as the point where any KPI degrades by more than 10% from the minimal-configuration baseline.</t>
      </section>

      <section anchor="test-12-2" numbered="true" toc="default">
        <name>Dynamic Autoscaling Response Time</name>
        <t><strong>Objective:</strong> To measure the time required for the fabric to accommodate dynamic scaling of inference worker pools <xref target="K8S-INF"/>.</t>
        <t><strong>Procedure:</strong> Starting from a stable serving state, trigger a scale-up event (e.g., adding 4 decode nodes). Measure: (a) time from scale trigger to new nodes being reachable via the fabric (fabric convergence time), (b) time from fabric convergence to the first KV cache transfer landing on new nodes, (c) time to reach steady-state throughput. Repeat for scale-down events.</t>
        <t><strong>Measurement:</strong> Report fabric convergence time (ms), first-transfer time (ms), and time to steady-state (ms) for scale-up and scale-down events. Report any packet loss or latency spikes during the scaling transition.</t>
      </section>

      <section anchor="test-12-3" numbered="true" toc="default">
        <name>Link Failure Convergence Impact on Serving</name>
        <t><strong>Objective:</strong> To measure the impact of fabric link failures on inference serving performance and the convergence time to restore full service.</t>
        <t><strong>Procedure:</strong> During sustained inference serving at 80% of SLO-bounded throughput, fail a single fabric link on: (a) a leaf-spine link carrying KV cache traffic, (b) a spine-spine link, (c) a link on the decode worker's leaf switch. Measure the duration of traffic disruption and time to restore pre-failure TTFT and TPS levels. Repeat the test for dual link failures.</t>
        <t><strong>Measurement:</strong> Report traffic disruption duration (ms), convergence time (ms), TTFT degradation during convergence (ms above baseline P99), TPS reduction during convergence (%), and time to full recovery (ms). The test MUST be repeated a minimum of 20 times per failure scenario.</t>
      </section>
    </section>

    <section anchor="test-cat9" numbered="true" toc="default">
      <name>Test Category 9: Soak and Stability</name>
      <t>
        Long-running inference serving deployments must maintain performance
        without degradation over time. This section defines soak tests adapted
        for inference workloads.
      </t>

      <section anchor="test-13-1" numbered="true" toc="default">
        <name>24-Hour Sustained Inference Load</name>
        <t><strong>Objective:</strong> To verify that the fabric maintains performance under continuous inference serving load for 24 hours.</t>
        <t><strong>Procedure:</strong> Configure the SUT-E at 80% of the SLO-bounded throughput determined in Test 11.1. Run a continuous request stream for 24 hours with a realistic prompt length distribution. Sample the following metrics every 15 minutes: TTFT P99, ITL P99, TPS_output, KV_xfer_latency P99, fabric link utilization, switch CPU/memory usage, NIC counters (RDMA retransmissions, QP errors), and PFC/ECN event counts.</t>
        <t><strong>Measurement:</strong> Report the trend of all sampled metrics over the 24-hour period. There SHOULD be zero NIC QP errors, zero routing flaps, and less than 1% variation in TTFT P99 over the test duration.</t>
      </section>

      <section anchor="test-13-2" numbered="true" toc="default">
        <name>KV Cache Memory Leak Detection</name>
        <t><strong>Objective:</strong> To detect memory leaks in the KV cache management subsystem that may manifest as fabric performance degradation over time.</t>
        <t><strong>Procedure:</strong> Monitor GPU memory, CPU memory, NIC registered memory regions, and RDMA memory region counts on all prefill and decode workers during the 24-hour soak test. Record the number of active KV cache pages, RDMA memory registrations, and pinned memory at each sampling interval.</t>
        <t><strong>Measurement:</strong> Report the trend of each monitored metric. Flag any monotonic increase as a potential leak. Report the maximum observed memory usage and the usage at the end of the 24-hour period.</t>
      </section>

      <section anchor="test-13-3" numbered="true" toc="default">
        <name>Long-Running Serving Stability</name>
        <t><strong>Objective:</strong> To verify that fabric-dependent components (NIC RDMA transport, switch buffer management, routing protocol state) remain stable under continuous inference serving.</t>
        <t><strong>Procedure:</strong> During the 24-hour soak test, monitor: NIC QP state transitions, switch buffer utilization trend, FEC error rate trend, BGP/OSPF adjacency stability, and RDMA retransmission rate. At the 12-hour mark, trigger a controlled perturbation (single link flap) and verify recovery.</t>
        <t><strong>Measurement:</strong> Report the count of any QP state transitions, maximum switch buffer utilization, FEC error trend, adjacency flap count, and RDMA retransmission count. Report the recovery time from the 12-hour link flap perturbation.</t>
      </section>
    </section>

    <section anchor="reporting" numbered="true" toc="default">
      <name>Reporting Format</name>
      <t>
        All test results MUST be reported following the conventions established
        in RFC 2544 Section 26. In addition, the following inference-specific
        reporting requirements apply:
      </t>
      <ul spacing="normal">
        <li><t><strong>System Configuration Report:</strong> The report MUST include the complete system configuration: model name and parameter count, parallelism strategy (TP, DP, EP, PP configuration for both prefill and decode pools), xPyD ratio, inference serving framework name and version, KV cache transfer library name and version, accelerator type and count, NIC type and firmware version, switch ASIC and software version, fabric topology, and link speeds.</t></li>
        <li><t><strong>Workload Characterization Report:</strong> The report MUST include the workload parameters: prompt length distribution (mean, P50, P99, distribution type), output length distribution, request arrival rate and distribution, number of concurrent requests, and prefix sharing percentage.</t></li>
        <li><t><strong>Results Reporting:</strong> For each test, results MUST include: the specific test identifier (e.g., Test 5.1), the DUT/SUT configuration tested, the number of trials, all measured KPI values with confidence intervals, and any anomalies observed. Graphical reports SHOULD follow the formats specified in each test section.</t></li>
      </ul>
      <table anchor="tab-reporting">
        <name>Reporting Format Requirements</name>
        <thead>
          <tr><th>Report Element</th><th>Format</th><th>Required?</th></tr>
        </thead>
        <tbody>
          <tr><td>System Configuration</td><td>Structured table per above</td><td>MUST</td></tr>
          <tr><td>Workload Parameters</td><td>Structured table per above</td><td>MUST</td></tr>
          <tr><td>KPI Summary Table</td><td>Table with all measured KPIs</td><td>MUST</td></tr>
          <tr><td>Latency Distribution Plots</td><td>CDF or histogram per test section</td><td>SHOULD</td></tr>
          <tr><td>Throughput vs. Scale Graphs</td><td>Line chart per test section</td><td>SHOULD</td></tr>
          <tr><td>Fabric Health Indicators</td><td>Table per Section 4.4</td><td>MUST</td></tr>
          <tr><td>Raw Data Appendix</td><td>Machine-readable format (CSV, JSON)</td><td>MAY</td></tr>
        </tbody>
      </table>
    </section>

    <section anchor="security" numbered="true" toc="default">
      <name>Security Considerations</name>
      <t>
        This document defines benchmarking methodologies for controlled
        laboratory testing. All tests MUST be conducted in isolated test
        environments that are not connected to production networks or the
        public Internet. The security considerations from <xref target="RFC2544"/>
        and <xref target="RFC6815"/> apply.
      </t>
      <t>
        Additionally, implementers SHOULD be aware that RDMA-based KV cache
        transfer provides direct memory access between hosts; all RDMA
        connections in the test environment MUST use authenticated QPs where
        supported. The test results themselves may reveal performance
        characteristics that could inform denial-of-service attack vectors;
        results SHOULD be treated as sensitive when applicable.
      </t>
    </section>

    <section anchor="iana" numbered="true" toc="default">
      <name>IANA Considerations</name>
      <t>This memo includes no request to IANA.</t>
    </section>

  </middle>

  <back>
    <references>
      <name>References</name>

      <references>
        <name>Normative References</name>
        <reference anchor="RFC2119" target="https://www.rfc-editor.org/info/rfc2119">
          <front>
            <title>Key words for use in RFCs to Indicate Requirement Levels</title>
            <author fullname="S. Bradner" initials="S." surname="Bradner"/>
            <date month="March" year="1997"/>
          </front>
          <seriesInfo name="BCP" value="14"/>
          <seriesInfo name="RFC" value="2119"/>
          <seriesInfo name="DOI" value="10.17487/RFC2119"/>
        </reference>
        <reference anchor="RFC8174" target="https://www.rfc-editor.org/info/rfc8174">
          <front>
            <title>Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words</title>
            <author fullname="B. Leiba" initials="B." surname="Leiba"/>
            <date month="May" year="2017"/>
          </front>
          <seriesInfo name="BCP" value="14"/>
          <seriesInfo name="RFC" value="8174"/>
          <seriesInfo name="DOI" value="10.17487/RFC8174"/>
        </reference>
        <reference anchor="RFC1242" target="https://www.rfc-editor.org/info/rfc1242">
          <front>
            <title>Benchmarking Terminology for Network Interconnect Devices</title>
            <author fullname="S. Bradner" initials="S." surname="Bradner"/>
            <date month="July" year="1991"/>
          </front>
          <seriesInfo name="RFC" value="1242"/>
          <seriesInfo name="DOI" value="10.17487/RFC1242"/>
        </reference>
        <reference anchor="RFC2544" target="https://www.rfc-editor.org/info/rfc2544">
          <front>
            <title>Benchmarking Methodology for Network Interconnect Devices</title>
            <author fullname="S. Bradner" initials="S." surname="Bradner"/>
            <author fullname="J. McQuaid" initials="J." surname="McQuaid"/>
            <date month="March" year="1999"/>
          </front>
          <seriesInfo name="RFC" value="2544"/>
          <seriesInfo name="DOI" value="10.17487/RFC2544"/>
        </reference>
        <reference anchor="RFC2889" target="https://www.rfc-editor.org/info/rfc2889">
          <front>
            <title>Benchmarking Methodology for LAN Switching Devices</title>
            <author fullname="R. Mandeville" initials="R." surname="Mandeville"/>
            <author fullname="J. Perser" initials="J." surname="Perser"/>
            <date month="August" year="2000"/>
          </front>
          <seriesInfo name="RFC" value="2889"/>
          <seriesInfo name="DOI" value="10.17487/RFC2889"/>
        </reference>
        <reference anchor="RFC6349" target="https://www.rfc-editor.org/info/rfc6349">
          <front>
            <title>Framework for TCP Throughput Testing</title>
            <author fullname="B. Constantine" initials="B." surname="Constantine"/>
            <author fullname="G. Forget" initials="G." surname="Forget"/>
            <author fullname="R. Geib" initials="R." surname="Geib"/>
            <author fullname="R. Schrage" initials="R." surname="Schrage"/>
            <date month="August" year="2011"/>
          </front>
          <seriesInfo name="RFC" value="6349"/>
          <seriesInfo name="DOI" value="10.17487/RFC6349"/>
        </reference>
      </references>

      <references>
        <name>Informative References</name>
        <reference anchor="RFC7432" target="https://www.rfc-editor.org/info/rfc7432">
          <front>
            <title>BGP MPLS-Based Ethernet VPN</title>
            <author fullname="A. Sajassi" initials="A." role="editor" surname="Sajassi"/>
            <date month="February" year="2015"/>
          </front>
          <seriesInfo name="RFC" value="7432"/>
          <seriesInfo name="DOI" value="10.17487/RFC7432"/>
        </reference>
        <reference anchor="RFC6815" target="https://www.rfc-editor.org/info/rfc6815">
          <front>
            <title>Applicability Statement for RFC 2544: Use on Production Networks Considered Harmful</title>
            <author fullname="S. Bradner" initials="S." surname="Bradner"/>
            <author fullname="K. Dubray" initials="K." surname="Dubray"/>
            <author fullname="J. McQuaid" initials="J." surname="McQuaid"/>
            <author fullname="A. Morton" initials="A." surname="Morton"/>
            <date month="November" year="2012"/>
          </front>
          <seriesInfo name="RFC" value="6815"/>
          <seriesInfo name="DOI" value="10.17487/RFC6815"/>
        </reference>
        <reference anchor="TRAINING-BENCH">
          <front>
            <title>Benchmarking Methodology for AI Training Network Fabrics</title>
            <author><organization>BMWG</organization></author>
            <date year="2026"/>
          </front>
          <seriesInfo name="Internet-Draft" value="draft-calabria-bmwg-ai-fabric-training-bench-00"/>
        </reference>
        <reference anchor="TERMINOLOGY">
          <front>
            <title>AI Fabric Benchmarking Terminology</title>
            <author><organization>BMWG</organization></author>
            <date year="2026"/>
          </front>
          <seriesInfo name="Internet-Draft" value="draft-calabria-bmwg-ai-fabric-terminology-00"/>
        </reference>
        <reference anchor="UEC-SPEC">
          <front>
            <title>UEC Specification 1.0</title>
            <author><organization>Ultra Ethernet Consortium</organization></author>
            <date year="2024"/>
          </front>
        </reference>
        <reference anchor="VLLM">
          <front>
            <title>Efficient Memory Management for Large Language Model Serving with PagedAttention</title>
            <author fullname="W. Kwon" initials="W." surname="Kwon"/>
            <date year="2023"/>
          </front>
          <refcontent>Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</refcontent>
        </reference>
        <reference anchor="DISTSERVE">
          <front>
            <title>DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</title>
            <author fullname="Y. Zhong" initials="Y." surname="Zhong"/>
            <date year="2024"/>
          </front>
          <refcontent>OSDI 2024</refcontent>
        </reference>
        <reference anchor="EP-COMM">
          <front>
            <title>DeepEP: An Efficient Expert-Parallel Communication Library</title>
            <author><organization/></author>
            <date year="2025"/>
          </front>
        </reference>
        <reference anchor="LMCACHE">
          <front>
            <title>LMCache: Hierarchical KV Cache Management for Inference</title>
            <author><organization>LMCache Project</organization></author>
            <date year="2025"/>
          </front>
        </reference>
        <reference anchor="SGLANG">
          <front>
            <title>SGLang: Efficient Execution of Structured Language Model Programs</title>
            <author><organization/></author>
            <date year="2024"/>
          </front>
        </reference>
        <reference anchor="K8S-INF">
          <front>
            <title>llm-d: Kubernetes-Native Distributed LLM Inference</title>
            <author><organization/></author>
            <date year="2025"/>
          </front>
        </reference>
      </references>
    </references>

    <section anchor="appendix-a" numbered="true" toc="default">
      <name>KPI-to-Test Mapping Summary</name>
      <t>
        The following table provides a cross-reference from each KPI defined
        in <xref target="kpi-framework"/> to the test(s) in which it is
        measured.
      </t>
      <table anchor="tab-kpi-mapping">
        <name>KPI-to-Test Mapping</name>
        <thead>
          <tr><th>KPI</th><th>Primary Test(s)</th><th>DUT/SUT</th></tr>
        </thead>
        <tbody>
          <tr><td>TTFT</td><td>6.1, 6.2, 10.1, 10.3</td><td>SUT-E</td></tr>
          <tr><td>ITL</td><td>10.2, 10.3, 10.4</td><td>SUT-E</td></tr>
          <tr><td>TPS_output</td><td>6.2, 11.1, 11.2, 11.3</td><td>SUT-E</td></tr>
          <tr><td>TPS_input</td><td>11.1</td><td>SUT-E</td></tr>
          <tr><td>Goodput</td><td>11.1, 11.3</td><td>SUT-E</td></tr>
          <tr><td>KV_xfer_latency</td><td>5.2, 5.3, 6.1, 6.4</td><td>DUT-N, DUT-PD</td></tr>
          <tr><td>KV_xfer_bandwidth</td><td>5.1, 5.3, 5.4</td><td>DUT-N, DUT-PD</td></tr>
          <tr><td>EP_alltoall_latency</td><td>7.1, 7.2, 7.3, 7.4</td><td>DUT-F</td></tr>
          <tr><td>EP_alltoall_bandwidth</td><td>7.1, 7.3</td><td>DUT-F</td></tr>
          <tr><td>Fabric_FCT</td><td>5.2, 5.3</td><td>DUT-F</td></tr>
          <tr><td>Buffer_utilization</td><td>8.1, 8.2</td><td>DUT-S</td></tr>
          <tr><td>ECN_marking_rate</td><td>8.1, 8.3</td><td>DUT-S</td></tr>
          <tr><td>PFC_frame_count</td><td>8.2, 8.4</td><td>DUT-S</td></tr>
          <tr><td>Link_utilization</td><td>5.3, 9.3, 12.1</td><td>DUT-F</td></tr>
          <tr><td>Packet_drop_rate</td><td>8.1, 8.2, 12.3</td><td>DUT-F</td></tr>
          <tr><td>Request_Rate</td><td>11.1</td><td>SUT-E</td></tr>
          <tr><td>Prefix Cache Hit Rate</td><td>9.2</td><td>SUT-E</td></tr>
          <tr><td>JFI (Decode Worker)</td><td>9.4</td><td>SUT-E</td></tr>
        </tbody>
      </table>
    </section>

    <section anchor="appendix-b" numbered="true" toc="default">
      <name>Inference Serving Framework Capability Categories (Informational)</name>
      <t>
        This appendix describes the inference serving framework capability
        categories relevant to AI fabric benchmarking. This appendix is
        intended to guide documentation of SUT-E configurations and is NOT
        normative. Implementers using a Software Workload Emulator (SUT-E
        tests) SHOULD document which of the following capabilities their
        serving framework supports.
      </t>
      <table anchor="tab-framework-caps">
        <name>Framework Capability Categories</name>
        <thead>
          <tr><th>Capability Category</th><th>Description</th><th>Relevance to Fabric Benchmarking</th></tr>
        </thead>
        <tbody>
          <tr><td>Disaggregated Prefill/Decode (PD)</td><td>Physical separation of prefill and decode execution across different accelerator pools</td><td>Determines whether DUT-PD topology tests apply (Section 6)</td></tr>
          <tr><td>KV Cache Transfer Protocol</td><td>Protocol and library used for prefill-to-decode KV state transfer (one-sided PUT, two-sided SEND/RECV, GPU-initiated)</td><td>Determines RDMA verb types under test and applicable frame formats (Appendix C)</td></tr>
          <tr><td>MoE Expert Parallelism (EP) Support</td><td>Distribution of MoE expert sub-networks across GPUs and AllToAll dispatch mode support</td><td>Determines whether MoE EP tests apply (Section 7)</td></tr>
          <tr><td>Continuous Batching</td><td>Dynamic request admission to active inference batches</td><td>Affects request arrival rate distributions and load balancing tests (Section 9)</td></tr>
          <tr><td>Prefix / KV Cache Sharing</td><td>Reuse of KV cache segments for requests with common prefixes</td><td>Determines applicability of prefix cache hit rate test (Section 9.2)</td></tr>
          <tr><td>RDMA Transport Support</td><td>Underlying transport(s) supported: RoCEv2, UET, or other</td><td>Must be documented; affects congestion management test interpretation (Section 8)</td></tr>
          <tr><td>GPU-Initiated Networking (GIN) Support</td><td>Ability for GPU threads to directly initiate RDMA operations without CPU involvement</td><td>Affects RDMA primitive choice in MoE dispatch tests (Section 7)</td></tr>
          <tr><td>Kubernetes / Orchestration Integration</td><td>Native support for container-based deployment and horizontal scaling</td><td>Relevant for autoscaling tests (Section 12.2)</td></tr>
          <tr><td>Maximum Reported Scale</td><td>Maximum cluster scale at which the framework has been validated</td><td>Documents applicability of fabric scale tests</td></tr>
        </tbody>
      </table>
      <t>
        NOTE: Implementers MUST document the specific framework name, version,
        and configuration in all test reports. Results obtained with different
        frameworks are not directly comparable; framework identity is a
        required reporting parameter per <xref target="reporting"/>.
      </t>
    </section>

    <section anchor="appendix-c" numbered="true" toc="default">
      <name>KV Cache Transfer Frame Format</name>
      <t>
        This appendix defines the reference frame formats for KV cache
        transfer benchmarking over RoCEv2. The frame format follows the
        standard RoCEv2 encapsulation with one-sided RDMA WRITE (PUT)
        operations.
      </t>
      <figure anchor="fig-roce-frame">
        <name>RoCEv2 KV Cache Transfer Frame (One-Sided RDMA WRITE)</name>
        <artwork align="left" name="" type="" alt=""><![CDATA[
    0                   1                   2                   3
    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |             Destination MAC Address (bytes 0-3)               |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |  Destination MAC (bytes 4-5)  |  Source MAC Address (0-1)     |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                 Source MAC Address (bytes 2-5)                |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |   EtherType = 0x0800/0x86DD   |  DSCP  |ECN|      ...        |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                                                               |
   |               IPv4/IPv6 Header (20 or 40 bytes)              |
   |                                                               |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |       Src Port (entropy)      |  Dst Port = 4791 (RoCEv2)    |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |          UDP Length           |          UDP Checksum         |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |  OpCode=RDMA_WRITE(0x0A) |SE|M|  Pad  |TVer|      PKey      |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |         Destination QP Number (24 bits)          |A| Reserved|
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |              Packet Sequence Number (PSN, 24 bits)            |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                                                               |
   |                RETH: Virtual Address (64 bits)                |
   |                                                               |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                  Remote Key (R_Key, 32 bits)                  |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                    DMA Length (32 bits)                       |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                                                               |
   |          KV Cache Payload (variable, up to MTU)               |
   |           (key/value attention state data)                    |
   |                                                               |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                       ICRC (4 bytes)                          |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        ]]></artwork>
      </figure>
      <t>
        Notes: The UDP Source Port SHOULD use entropy-based values for ECMP
        load distribution. The RETH (RDMA Extended Transport Header) carries
        the remote virtual address, remote key, and DMA length for the
        one-sided WRITE operation. For KV cache transfers, the DMA Length
        field indicates the size of the KV cache block being transferred.
        Typical MTU for RoCEv2 is 4096 bytes; larger KV cache blocks (e.g.,
        64 KB pages) are segmented into multiple packets by the NIC. For
        PUT-with-signal operations, the last packet in the transfer includes
        an RDMA WRITE with Immediate Data (OpCode 0x0B) to signal completion
        to the decode worker.
      </t>
    </section>

    <section anchor="appendix-d" numbered="true" toc="default">
      <name>MoE AllToAll Communication Pattern</name>
      <t>
        This appendix describes the AllToAll communication pattern used for
        MoE expert parallelism dispatch and its fabric-level traffic
        characteristics. In a Mixture-of-Experts model with M total experts
        distributed across N GPUs (each GPU holds M/N experts), a single MoE
        layer forward pass generates an AllToAll communication pattern where
        each GPU sends a variable-size payload to every other GPU.
      </t>
      <table anchor="tab-moe-dispatch">
        <name>MoE Dispatch Traffic Characteristics by Mode</name>
        <thead>
          <tr><th>Parameter</th><th>Normal Dispatch (Prefill)</th><th>Low-Latency Dispatch (Decode)</th></tr>
        </thead>
        <tbody>
          <tr><td>Batch Size</td><td>128 - 512 tokens</td><td>1 - 16 tokens</td></tr>
          <tr><td>Payload per GPU pair</td><td>Variable (depends on routing)</td><td>Fixed (padded to max)</td></tr>
          <tr><td>Shape Compatibility</td><td>Dynamic (symbolic)</td><td>Static (CUDA Graph)</td></tr>
          <tr><td>QP Parallelism</td><td>24 QPs per connection</td><td>8 - 16 QPs per connection</td></tr>
          <tr><td>RDMA Primitive</td><td>Two-sided SEND/RECV or one-sided PUT</td><td>One-sided PUT (GPU-direct RDMA, GIN)</td></tr>
          <tr><td>GPU Initiation</td><td>CPU-initiated or GIN</td><td>GIN (device-initiated, GPU-to-NIC direct)</td></tr>
          <tr><td>Typical per-dispatch size</td><td>1 - 10 MB aggregate</td><td>10 KB - 1 MB aggregate</td></tr>
          <tr><td>Dispatch Frequency</td><td>Once per MoE layer (prefill)</td><td>Once per MoE layer per token (decode)</td></tr>
          <tr><td>Latency Target</td><td>&lt; 1 ms per dispatch</td><td>&lt; 200 us per dispatch</td></tr>
        </tbody>
      </table>
      <t>
        For a representative large-scale MoE model (example: 256 experts,
        top-2 routing, hidden_dim=7168, EP=96 across 12 nodes), the
        inter-node traffic per MoE layer dispatch is approximately:
      </t>
      <ul spacing="normal">
        <li>Normal Dispatch (prefill, batch=256): 256 * 2 * 7168 * 2 bytes / 96 GPUs = ~76 KB per GPU pair, ~870 MB aggregate across all pairs.</li>
        <li>Low-Latency Dispatch (decode, batch=8): 8 * 2 * 7168 * 2 bytes / 96 GPUs = ~2.4 KB per GPU pair, ~27 MB aggregate.</li>
      </ul>
      <t>
        With 61 MoE layers and a decode iteration time target of ~30 ms, the
        decode phase requires 61 AllToAll dispatches within 30 ms, yielding
        ~2,000 dispatches per second per decode step, consuming approximately
        54 GB/s aggregate inter-node bandwidth for the Low-Latency Dispatch
        path.
      </t>
    </section>

    <section anchor="acknowledgements" numbered="false" toc="default">
      <name>Acknowledgements</name>
      <t>
        Contributions and review are solicited from the BMWG mailing list
        (bmwg@ietf.org) and the broader AI networking community. The BMWG
        chairs and Area Director are identified at
        https://datatracker.ietf.org/group/bmwg/about/.
      </t>
    </section>

  </back>
</rfc>
